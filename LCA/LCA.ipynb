{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br><br></br><br></br><br></br><br></br><br></br><br></br>\n",
    "## <center>Artificial Intelligence: Knowledge Representation and Planning</center>\n",
    "### <center>CM0472: Assignment 3 â€“ Clustering</center>\n",
    "<br></br><br></br><br></br><br></br><br></br><br></br><br></br><br></br>\n",
    "###### JABER RAHIMIFARD [875545]\n",
    "###### AREF ENAYATI [882341]\n",
    "<br></br><br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "Perform classification of the  Semeion Handwritten Digit data set using:\n",
    "\n",
    "    -Latent class analysis;\n",
    "    -Mean shift;\n",
    "    -Normalized cut.\n",
    "    \n",
    "__Mean shift__ and __Normalized Cut__ assume that the images are vectors in a 256 dimensional Euclidean space.\n",
    "\n",
    "\n",
    "\n",
    "Provide the code and the extracted clusters as the number of clusters k varies __from 5 to 15__, for __LCA and normalized-cut__, while for __Mean shift__ vary the kernel width. For each value of k (or kernel width) provide the value of the Rand index:<br></br><br></br>\n",
    "__<center>R=2(a+b)/(n(n-1))</center>__\n",
    "\n",
    "where\n",
    "    <br>n is the number of images in the dataset.</br>\n",
    "    <br>a is the number of pairs of images that represent the same digit and that are clustered together.</br>\n",
    "    <br> is the number of pairs of images that represent different digits and that are placed in different clusters.</br>\n",
    "Explain the differences between the three models.\n",
    "\n",
    "__Tip:__ Bernoulli models can be visualized as a greyscale image to inspect the learned model.\n",
    "\n",
    "<br></br><br></br><br></br><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this assignment we perform classification using 3 algorithms __LCA__, __Mean Shift__, and __Normalized Cut__ on the Semeion Handwritten Digit dataset.\n",
    "<br>We modify the number of cluster in each cluster and the widthBand in case using mean shift algorithm.</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.special import logsumexp\n",
    "from time import time\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    data = np.loadtxt('./semeion.data', dtype=np.int8)\n",
    "    return data[:, :256], data[:, 256:]\n",
    "\n",
    "\n",
    "def one_hot_decode(y: np.array):\n",
    "    return np.argmax(y, axis=1)\n",
    "\n",
    "\n",
    "def get_data_transformed():\n",
    "    data = np.loadtxt('./semeion.data', dtype=np.int8)\n",
    "    return data[:, :256], one_hot_decode(data[:, 256:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake class for no dim. red.\n",
    "class NoDR:\n",
    "    def __init__(self, n_components):\n",
    "        pass\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        return X.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "# vector containing the number of clusters\n",
    "ks = range(5, 16)\n",
    "# vector containing the dimensions for dimensionality reduction\n",
    "ds = (2, 64, 128, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_no_underflow(x):\n",
    "    \"\"\"Computes log without underflow\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float or numpy.ndarray\n",
    "    Returns\n",
    "    -------\n",
    "    log(x) if x > 0 else log eps\n",
    "    \"\"\"\n",
    "    return np.log(np.clip(x, 1e-15, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _estimate_bernoulli_parameters(X, resp):\n",
    "    \"\"\"Estimate the Bernoulli distribution parameters.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The input data array.\n",
    "    resp : array-like, shape (n_samples, n_components)\n",
    "        The responsibilities for each data sample in X.\n",
    "    Returns\n",
    "    -------\n",
    "    nk : array-like, shape (n_components,)\n",
    "        The numbers of data samples in the current components.\n",
    "    means : array-like, shape (n_components, n_features)\n",
    "        The centers of the current components.\n",
    "    \"\"\"\n",
    "    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n",
    "    means = np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "    return nk, means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _estimate_log_bernoulli_prob(X, means):\n",
    "    \"\"\"Estimate the log Bernoulli probability.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "    means : array-like, shape (n_components, n_features)\n",
    "    Returns\n",
    "    -------\n",
    "    log_prob : array, shape (n_samples, n_components)\n",
    "    \"\"\"\n",
    "    return (X.dot(_log_no_underflow(means).T) +\n",
    "            (1 - X).dot(_log_no_underflow(1 - means).T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bernoulli_Mixture:\n",
    "    def __init__(self, n_components, max_iter=500, tol=1e-3):\n",
    "        '''Mixture of Bernulli Model\n",
    "        pi : mixture weights / mixing coefficients\n",
    "        mu : Bernoulli parameters / means\n",
    "        gamma : responsibilities / expected values z_nk, sample n comes from component k\n",
    "        '''\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'BMM'\n",
    "\n",
    "    def fit(self, X, init_kmeans=True):\n",
    "        \"\"\"Estimate model parameters using X and predict the labels for X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            List of n_features-dimensional data points. Each row\n",
    "            corresponds to a single data point.\n",
    "        Returns\n",
    "        -------\n",
    "        labels : array, shape (n_samples,)\n",
    "            Component labels.\n",
    "        \"\"\"\n",
    "\n",
    "        self.converged_ = False\n",
    "\n",
    "        n_samples, _ = X.shape\n",
    "        random_state = 42\n",
    "        if init_kmeans:\n",
    "            resp = np.zeros((n_samples, self.n_components))\n",
    "            label = (\n",
    "                KMeans(n_clusters=self.n_components, n_init=1, random_state=random_state)\n",
    "                    .fit(X)\n",
    "                    .labels_\n",
    "            )\n",
    "            resp[np.arange(n_samples), label] = 1\n",
    "        else:\n",
    "            resp = random_state.rand(n_samples, self.n_components)\n",
    "            resp /= resp.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        self._initialize(X, resp)\n",
    "\n",
    "        lower_bound = -np.inf\n",
    "\n",
    "        for n_iter in range(1, self.max_iter + 1):\n",
    "            prev_lower_bound = lower_bound\n",
    "\n",
    "            log_prob_norm, log_resp = self._e_step(X)\n",
    "            self._m_step(X, log_resp)\n",
    "            lower_bound = log_prob_norm\n",
    "\n",
    "            change = lower_bound - prev_lower_bound\n",
    "\n",
    "            if abs(change) < self.tol:\n",
    "                self.converged_ = True\n",
    "                break\n",
    "\n",
    "        if not self.converged_:\n",
    "            raise ValueError('Not converged')\n",
    "\n",
    "        _, log_resp = self._e_step(X)\n",
    "\n",
    "        self.labels_ = log_resp.argmax(axis=1)\n",
    "\n",
    "    def _e_step(self, X):\n",
    "        \"\"\"E step.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        log_prob_norm : float\n",
    "            Mean of the logarithms of the probabilities of each sample in X\n",
    "        log_responsibility : array, shape (n_samples, n_components)\n",
    "            Logarithm of the posterior probabilities (or responsibilities) of\n",
    "            the point of each sample in X.\n",
    "        \"\"\"\n",
    "        log_prob_norm, log_resp = self._estimate_log_prob_resp(X)\n",
    "        return np.mean(log_prob_norm), log_resp\n",
    "\n",
    "    def _estimate_weighted_log_prob(self, X):\n",
    "        \"\"\"Estimate the weighted log-probabilities, log P(X | Z) + log weights.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        weighted_log_prob : array, shape (n_samples, n_component)\n",
    "        \"\"\"\n",
    "        return self._estimate_log_prob(X) + self._estimate_log_weights()\n",
    "\n",
    "    def _estimate_log_prob_resp(self, X):\n",
    "        \"\"\"Estimate log probabilities and responsibilities for each sample.\n",
    "        Compute the log probabilities, weighted log probabilities per\n",
    "        component and responsibilities for each sample in X with respect to\n",
    "        the current state of the model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        log_prob_norm : array, shape (n_samples,)\n",
    "            log p(X)\n",
    "        log_responsibilities : array, shape (n_samples, n_components)\n",
    "            logarithm of the responsibilities\n",
    "        \"\"\"\n",
    "        weighted_log_prob = self._estimate_weighted_log_prob(X)\n",
    "        log_prob_norm = logsumexp(weighted_log_prob, axis=1)\n",
    "        with np.errstate(under=\"ignore\"):\n",
    "            # ignore underflow\n",
    "            log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]\n",
    "        return log_prob_norm, log_resp\n",
    "\n",
    "    def _initialize(self, X, resp):\n",
    "        \"\"\"Initialization of the Bernoulli mixture parameters.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        resp : array-like, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        n_samples, n_dim = X.shape\n",
    "\n",
    "        weights, means = _estimate_bernoulli_parameters(X, resp)\n",
    "        weights /= n_samples\n",
    "\n",
    "        self.weights_ = weights\n",
    "        self.means_ = means\n",
    "\n",
    "    def _m_step(self, X, log_resp):\n",
    "        \"\"\"M step.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        log_resp : array-like, shape (n_samples, n_components)\n",
    "            Logarithm of the posterior probabilities (or responsibilities) of\n",
    "            the point of each sample in X.\n",
    "        \"\"\"\n",
    "        n_samples, _ = X.shape\n",
    "        self.weights_, self.means_ = (\n",
    "            _estimate_bernoulli_parameters(X, np.exp(log_resp))\n",
    "        )\n",
    "        self.weights_ /= n_samples\n",
    "\n",
    "    def _estimate_log_prob(self, X):\n",
    "        return _estimate_log_bernoulli_prob(X, self.means_)\n",
    "\n",
    "    def _estimate_log_weights(self):\n",
    "        return np.log(self.weights_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LCA(p=False):\n",
    "    '''Results for parametric clustering models'''\n",
    "    # results will be inserted into a DataFrame\n",
    "    columns = ('clustering_model', 'dim_red_model', 'd', 'k', 'time', 'ARI', 'RI')\n",
    "    rows = []\n",
    "\n",
    "    # read data and split\n",
    "    X, y = get_data_transformed()\n",
    "    # loop clusteting model\n",
    "    for k in ks:\n",
    "        for d in ds:\n",
    "            if d != 256:\n",
    "                embedding_model = PCA\n",
    "            else:\n",
    "                embedding_model = NoDR\n",
    "            start_time = time()\n",
    "            embedding = embedding_model(n_components=d)  # DR model\n",
    "            X_transformed = embedding.fit_transform(X)\n",
    "            model = Bernoulli_Mixture(k)  # clustering model\n",
    "            model.fit(X_transformed)\n",
    "            elapsed = time() - start_time\n",
    "            title = f'Results for {model} on {d}-{embedding_model.__name__} - k={k} ({elapsed:.02f}s)'\n",
    "            print(title)\n",
    "            # RI score\n",
    "            ari, ri = print_rand(y, model.labels_)\n",
    "            # plot results\n",
    "            if p: plot2D(X_transformed, model.labels_, title)\n",
    "            result = (str(model), embedding_model.__name__, d, k, elapsed, ari, ri)\n",
    "            rows.append(result)\n",
    "    # save results to file\n",
    "    df = pd.DataFrame(data=rows, columns=columns)\n",
    "    print(df)\n",
    "    df.to_csv('LCA.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rand(y_true, y_pred):\n",
    "    'Print and return the Rand Indexes'\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "    ri = rand_score(y_true, y_pred)\n",
    "    print('adjusted_rand_score', round(ari * 100, 2), '%')\n",
    "    print('rand_score', round(ri * 100, 2), '%', end='\\n\\n')\n",
    "    return ari, ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_analysis():\n",
    "    df = pd.read_csv('LCA.csv')\n",
    "\n",
    "    dfg_model = df.groupby('clustering_model').mean()[['time', 'ARI', 'RI']]\n",
    "    print('averages by clustering model:')\n",
    "    print(dfg_model.sort_values('ARI', ascending=False))\n",
    "    print()\n",
    "\n",
    "    dfg_k = df.groupby('k').mean()[['time', 'ARI', 'RI']]\n",
    "    print('averages by k:')\n",
    "    print(dfg_k.sort_values('ARI', ascending=False))\n",
    "    print()\n",
    "\n",
    "    dfg_dr = df.groupby('dim_red_model').mean()[['time', 'ARI', 'RI']]\n",
    "    print('averages by DR model:')\n",
    "    print(dfg_dr.sort_values('ARI', ascending=False))\n",
    "    print()\n",
    "\n",
    "    dfg_both = df.groupby(['clustering_model', 'dim_red_model']).mean()[['time', 'ARI', 'RI']]\n",
    "    print('averages by clustering & DR model:')\n",
    "    print(dfg_both.sort_values('ARI', ascending=False))\n",
    "    print()\n",
    "\n",
    "    print('best ARI achieved by:')\n",
    "    print(df.sort_values('ARI', ascending=False).head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    LCA()\n",
    "    results_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for BMM on 2-PCA - k=5 (0.02s)\n",
      "adjusted_rand_score 16.07 %\n",
      "rand_score 72.39 %\n",
      "\n",
      "Results for BMM on 64-PCA - k=5 (0.07s)\n",
      "adjusted_rand_score 23.04 %\n",
      "rand_score 79.41 %\n",
      "\n",
      "Results for BMM on 128-PCA - k=5 (0.20s)\n",
      "adjusted_rand_score 21.78 %\n",
      "rand_score 79.42 %\n",
      "\n",
      "Results for BMM on 256-NoDR - k=5 (0.09s)\n",
      "adjusted_rand_score 25.26 %\n",
      "rand_score 79.95 %\n",
      "\n",
      "Results for BMM on 2-PCA - k=6 (0.04s)\n",
      "adjusted_rand_score 17.13 %\n",
      "rand_score 75.92 %\n",
      "\n",
      "Results for BMM on 64-PCA - k=6 (0.08s)\n",
      "adjusted_rand_score 27.1 %\n",
      "rand_score 82.26 %\n",
      "\n",
      "Results for BMM on 128-PCA - k=6 (0.15s)\n",
      "adjusted_rand_score 22.84 %\n",
      "rand_score 81.7 %\n",
      "\n",
      "Results for BMM on 256-NoDR - k=6 (0.34s)\n",
      "adjusted_rand_score 30.68 %\n",
      "rand_score 83.59 %\n",
      "\n",
      "Results for BMM on 2-PCA - k=7 (0.03s)\n",
      "adjusted_rand_score 17.06 %\n",
      "rand_score 76.77 %\n",
      "\n",
      "Results for BMM on 64-PCA - k=7 (0.07s)\n",
      "adjusted_rand_score 27.11 %\n",
      "rand_score 84.03 %\n",
      "\n",
      "Results for BMM on 128-PCA - k=7 (0.20s)\n",
      "adjusted_rand_score 26.95 %\n",
      "rand_score 84.24 %\n",
      "\n",
      "Results for BMM on 256-NoDR - k=7 (0.13s)\n",
      "adjusted_rand_score 34.04 %\n",
      "rand_score 84.05 %\n",
      "\n",
      "Results for BMM on 2-PCA - k=8 (0.03s)\n",
      "adjusted_rand_score 17.54 %\n",
      "rand_score 76.57 %\n",
      "\n",
      "Results for BMM on 64-PCA - k=8 (0.09s)\n",
      "adjusted_rand_score 27.24 %\n",
      "rand_score 85.16 %\n",
      "\n",
      "Results for BMM on 128-PCA - k=8 (0.18s)\n",
      "adjusted_rand_score 27.31 %\n",
      "rand_score 85.37 %\n",
      "\n",
      "Results for BMM on 256-NoDR - k=8 (0.20s)\n",
      "adjusted_rand_score 35.04 %\n",
      "rand_score 86.3 %\n",
      "\n",
      "Results for BMM on 2-PCA - k=9 (0.04s)\n",
      "adjusted_rand_score 17.16 %\n",
      "rand_score 75.87 %\n",
      "\n",
      "Results for BMM on 64-PCA - k=9 (0.07s)\n",
      "adjusted_rand_score 33.39 %\n",
      "rand_score 87.15 %\n",
      "\n",
      "Results for BMM on 128-PCA - k=9 (0.18s)\n",
      "adjusted_rand_score 30.41 %\n",
      "rand_score 86.81 %\n",
      "\n",
      "Results for BMM on 256-NoDR - k=9 (0.17s)\n",
      "adjusted_rand_score 41.09 %\n",
      "rand_score 88.44 %\n",
      "\n",
      "Results for BMM on 2-PCA - k=10 (0.03s)\n",
      "adjusted_rand_score 17.38 %\n",
      "rand_score 76.02 %\n",
      "\n",
      "Results for BMM on 64-PCA - k=10 (0.08s)\n",
      "adjusted_rand_score 35.45 %\n",
      "rand_score 88.19 %\n",
      "\n",
      "Results for BMM on 128-PCA - k=10 (0.24s)\n",
      "adjusted_rand_score 33.01 %\n",
      "rand_score 87.78 %\n",
      "\n",
      "Results for BMM on 256-NoDR - k=10 (0.27s)\n",
      "adjusted_rand_score 45.81 %\n",
      "rand_score 90.09 %\n",
      "\n",
      "Results for BMM on 2-PCA - k=11 (0.04s)\n",
      "adjusted_rand_score 18.33 %\n",
      "rand_score 78.96 %\n",
      "\n",
      "Results for BMM on 64-PCA - k=11 (0.09s)\n",
      "adjusted_rand_score 34.87 %\n",
      "rand_score 88.58 %\n",
      "\n",
      "Results for BMM on 128-PCA - k=11 (0.23s)\n",
      "adjusted_rand_score 31.08 %\n",
      "rand_score 87.86 %\n",
      "\n",
      "Results for BMM on 256-NoDR - k=11 (0.15s)\n",
      "adjusted_rand_score 46.38 %\n",
      "rand_score 90.59 %\n",
      "\n",
      "Results for BMM on 2-PCA - k=12 (0.05s)\n",
      "adjusted_rand_score 18.28 %\n",
      "rand_score 79.03 %\n",
      "\n",
      "Results for BMM on 64-PCA - k=12 (0.10s)\n",
      "adjusted_rand_score 29.61 %\n",
      "rand_score 88.16 %\n",
      "\n",
      "Results for BMM on 128-PCA - k=12 (0.15s)\n",
      "adjusted_rand_score 29.99 %\n",
      "rand_score 88.15 %\n",
      "\n",
      "Results for BMM on 256-NoDR - k=12 (0.27s)\n",
      "adjusted_rand_score 43.42 %\n",
      "rand_score 90.51 %\n",
      "\n",
      "Results for BMM on 2-PCA - k=13 (0.04s)\n",
      "adjusted_rand_score 17.63 %\n",
      "rand_score 76.68 %\n",
      "\n",
      "Results for BMM on 64-PCA - k=13 (0.09s)\n",
      "adjusted_rand_score 29.17 %\n",
      "rand_score 88.49 %\n",
      "\n",
      "Results for BMM on 128-PCA - k=13 (0.16s)\n",
      "adjusted_rand_score 36.14 %\n",
      "rand_score 89.55 %\n",
      "\n",
      "Results for BMM on 256-NoDR - k=13 (0.27s)\n",
      "adjusted_rand_score 42.54 %\n",
      "rand_score 90.67 %\n",
      "\n",
      "Results for BMM on 2-PCA - k=14 (0.04s)\n",
      "adjusted_rand_score 18.6 %\n",
      "rand_score 79.49 %\n",
      "\n",
      "Results for BMM on 64-PCA - k=14 (0.11s)\n",
      "adjusted_rand_score 35.65 %\n",
      "rand_score 89.78 %\n",
      "\n",
      "Results for BMM on 128-PCA - k=14 (0.21s)\n",
      "adjusted_rand_score 35.56 %\n",
      "rand_score 89.83 %\n",
      "\n",
      "Results for BMM on 256-NoDR - k=14 (0.23s)\n",
      "adjusted_rand_score 42.91 %\n",
      "rand_score 90.93 %\n",
      "\n",
      "Results for BMM on 2-PCA - k=15 (0.04s)\n",
      "adjusted_rand_score 18.27 %\n",
      "rand_score 79.96 %\n",
      "\n",
      "Results for BMM on 64-PCA - k=15 (0.12s)\n",
      "adjusted_rand_score 36.09 %\n",
      "rand_score 90.1 %\n",
      "\n",
      "Results for BMM on 128-PCA - k=15 (0.18s)\n",
      "adjusted_rand_score 37.02 %\n",
      "rand_score 90.29 %\n",
      "\n",
      "Results for BMM on 256-NoDR - k=15 (0.21s)\n",
      "adjusted_rand_score 41.44 %\n",
      "rand_score 90.89 %\n",
      "\n",
      "   clustering_model dim_red_model    d   k      time       ARI        RI\n",
      "0               BMM           PCA    2   5  0.022907  0.160668  0.723939\n",
      "1               BMM           PCA   64   5  0.065825  0.230438  0.794105\n",
      "2               BMM           PCA  128   5  0.198469  0.217759  0.794246\n",
      "3               BMM          NoDR  256   5  0.086769  0.252595  0.799484\n",
      "4               BMM           PCA    2   6  0.040890  0.171296  0.759161\n",
      "5               BMM           PCA   64   6  0.078790  0.270987  0.822641\n",
      "6               BMM           PCA  128   6  0.145612  0.228441  0.817042\n",
      "7               BMM          NoDR  256   6  0.338096  0.306814  0.835876\n",
      "8               BMM           PCA    2   7  0.033909  0.170576  0.767723\n",
      "9               BMM           PCA   64   7  0.067818  0.271133  0.840284\n",
      "10              BMM           PCA  128   7  0.200464  0.269483  0.842450\n",
      "11              BMM          NoDR  256   7  0.126662  0.340445  0.840507\n",
      "12              BMM           PCA    2   8  0.025930  0.175383  0.765696\n",
      "13              BMM           PCA   64   8  0.085771  0.272377  0.851599\n",
      "14              BMM           PCA  128   8  0.181515  0.273119  0.853710\n",
      "15              BMM          NoDR  256   8  0.201460  0.350395  0.862987\n",
      "16              BMM           PCA    2   9  0.044880  0.171587  0.758721\n",
      "17              BMM           PCA   64   9  0.074800  0.333885  0.871523\n",
      "18              BMM           PCA  128   9  0.176532  0.304128  0.868122\n",
      "19              BMM          NoDR  256   9  0.168549  0.410902  0.884384\n",
      "20              BMM           PCA    2  10  0.031916  0.173841  0.760246\n",
      "21              BMM           PCA   64  10  0.077792  0.354495  0.881865\n",
      "22              BMM           PCA  128  10  0.239361  0.330111  0.877750\n",
      "23              BMM          NoDR  256  10  0.265292  0.458088  0.900913\n",
      "24              BMM           PCA    2  11  0.037897  0.183329  0.789607\n",
      "25              BMM           PCA   64  11  0.093698  0.348671  0.885791\n",
      "26              BMM           PCA  128  11  0.231381  0.310833  0.878646\n",
      "27              BMM          NoDR  256  11  0.151594  0.463818  0.905877\n",
      "28              BMM           PCA    2  12  0.052858  0.182835  0.790255\n",
      "29              BMM           PCA   64  12  0.096741  0.296097  0.881590\n",
      "30              BMM           PCA  128  12  0.148603  0.299861  0.881527\n",
      "31              BMM          NoDR  256  12  0.274266  0.434214  0.905120\n",
      "32              BMM           PCA    2  13  0.044881  0.176273  0.766835\n",
      "33              BMM           PCA   64  13  0.089761  0.291683  0.884906\n",
      "34              BMM           PCA  128  13  0.163562  0.361437  0.895518\n",
      "35              BMM          NoDR  256  13  0.270278  0.425427  0.906664\n",
      "36              BMM           PCA    2  14  0.039894  0.185972  0.794861\n",
      "37              BMM           PCA   64  14  0.112698  0.356517  0.897775\n",
      "38              BMM           PCA  128  14  0.212431  0.355598  0.898261\n",
      "39              BMM          NoDR  256  14  0.226397  0.429084  0.909278\n",
      "40              BMM           PCA    2  15  0.042885  0.182745  0.799564\n",
      "41              BMM           PCA   64  15  0.120677  0.360913  0.900968\n",
      "42              BMM           PCA  128  15  0.179522  0.370235  0.902880\n",
      "43              BMM          NoDR  256  15  0.206449  0.414427  0.908878\n",
      "averages by clustering model:\n",
      "                      time       ARI       RI\n",
      "clustering_model                             \n",
      "BMM               0.131284  0.293839  0.84454\n",
      "\n",
      "averages by k:\n",
      "        time       ARI        RI\n",
      "k                               \n",
      "15  0.137383  0.332080  0.878072\n",
      "14  0.147855  0.331793  0.875044\n",
      "10  0.153590  0.329134  0.855193\n",
      "11  0.128643  0.326663  0.864980\n",
      "13  0.142120  0.313705  0.863481\n",
      "9   0.116190  0.305125  0.845688\n",
      "12  0.143117  0.303252  0.864623\n",
      "8   0.123669  0.267818  0.833498\n",
      "7   0.107213  0.262909  0.822741\n",
      "6   0.150847  0.244385  0.808680\n",
      "5   0.093492  0.215365  0.777944\n",
      "\n",
      "averages by DR model:\n",
      "                   time       ARI        RI\n",
      "dim_red_model                              \n",
      "NoDR           0.210528  0.389655  0.878179\n",
      "PCA            0.104869  0.261900  0.833328\n",
      "\n",
      "averages by clustering & DR model:\n",
      "                                    time       ARI        RI\n",
      "clustering_model dim_red_model                              \n",
      "BMM              NoDR           0.210528  0.389655  0.878179\n",
      "                 PCA            0.104869  0.261900  0.833328\n",
      "\n",
      "best ARI achieved by:\n",
      "   clustering_model dim_red_model    d   k      time       ARI        RI\n",
      "27              BMM          NoDR  256  11  0.151594  0.463818  0.905877\n",
      "23              BMM          NoDR  256  10  0.265292  0.458088  0.900913\n",
      "31              BMM          NoDR  256  12  0.274266  0.434214  0.905120\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
